---
description: Intelligence-driven workflow for Python chapters (12-29). Reads constitution + chapter-index to derive audience/complexity/prerequisites automatically. Asks only targeted questions when genuinely ambiguous. Chains /sp.specify â†’ /sp.plan â†’ /sp.tasks â†’ /sp.implement with validation gates.
---

# /sp.python-chapter: Intelligence-Driven Python Chapter Workflow

**Purpose**: Design a complete Python chapter (12-29) using **vertical intelligence** (constitution + chapter-index + skills) to automatically derive context. No hardcoded questions - the command reads authoritative sources and asks only what's genuinely ambiguous. Chains full workflow (Spec â†’ Plan â†’ Tasks â†’ Implement â†’ Validate) with approval gates.

**Intelligence Sources**:
- Constitution: Target audience, philosophy, learning patterns
- Chapter Index: Exact title (THE ANCHOR), part, prerequisites
- Skills Library: Available domain skills for this chapter
- Context Materials: Existing pedagogical patterns (if available)

**Adaptive Questions**: 0-3 targeted questions based on what intelligence can't derive (NOT hardcoded 4 questions).

## User Input

```text
$ARGUMENTS
```

## VERTICAL INTELLIGENCE: AIDD-Driven Python Teaching

Before orchestration begins, understand what makes Python chapters effective in the AI-native era:

### Core Principle: Specification-First, Validation-First, AI Partnership

Students don't memorize Python syntax. Instead:

1. **Understand the concept** (plain language explanation)
2. **See minimal code** (what it does in action)
3. **Ask their AI** (explore through dialogue with Claude Code/Gemini CLI)
4. **Extract insight** (why this matters for thinking, not just coding)

### AI-Native Learning for Part 4 Students

**Traditional Programming Teaching**:
- "Memorize Python syntax"
- "Here are all 47 string methods"
- Syntax-first (memorize, then apply)

**AI-Native Learning Pattern** (Part 4: Chapters 12-29):
- **Describe Intent**: Use type hints and clear code to communicate what data means
- **Explore with AI**: Ask AI questions to understand concepts (not memorize docs)
- **Validate Together**: Use isinstance(), type(), and tests to check understanding
- **Learn from Errors**: When errors occur, ask AI "why?" and learn the pattern

**Note on AIDD**: Students in Chapters 1-11 learned AIDD principles (Specification â†’ Generation â†’ Execution â†’ Reflection). Part 4 applies these principles to learning Python, using the beginner-friendly "AI-Native Learning" framing. Students don't write formal specifications yet (that's Part 5+), but they DO describe intent through type hints and clear code structure.

### Teaching Pattern (Every Concept)

```markdown
## 1. [Concept Name] â€” [Why it matters]

**What it is:**
Plain-language explanation (2-3 sentences).

### ğŸ’» Code Idea

\`\`\`python
# Minimal code showing the concept
# Focus on WHAT it does
\`\`\`

### ğŸ¤– Think With Your AI

> "What does this do?"
>
> "What changes if we...?"
>
> "How would you use this to...?"

### ğŸ§  The Reasoning Pattern

[Why this concept matters for thinking, not just coding]
```

**Example:**

```markdown
## 1. Variables â€” Storing Data

**What it is:**
A variable names a value so your program can remember it.

### ğŸ’» Code Idea

\`\`\`python
name = "Alex"
score = 95
\`\`\`

### ğŸ¤– Think With Your AI

> "Why do we need variables instead of just using 95?"
>
> "What breaks if we forget to name a value?"
>
> "How do AI agents use variables to track context?"

### ğŸ§  The Reasoning Pattern

Programs need memory. Variables let you say "remember this as X"â€”
exactly how reasoning chains in AI maintain state.
```

---

## Python Standards (Chapters 12-29)

**Version:** 3.14+ (always use latest stable release from https://www.python.org/downloads/)
**Syntax:** f-strings only, match/case (17+), modern types (`list[int]`, `X | None`)
**Type hints:** Core (Ch 13) â†’ Gradual Application (14-26) â†’ Mandatory (27+)
**Note on Type Hints:** Modern Python treats type hints as essential for clarity and specification-first thinking, not optional features. Integrate from Chapter 13 onwards.

**Security (non-negotiable):**
- âŒ No `eval()`, `shell=True`, hardcoded secrets
- âœ… Environment variables, input validation, modern patterns

---

## CRITICAL DESIGN RULES

### Rule 1: USER INTENT IS AUTHORITY

**Never override user input:**
- User says "beginner" â†’ Make A1-A2 (NOT A2-B1)
- User says "just variables" â†’ Only variables (NOT + functions + loops)
- User says "absolute beginners" â†’ 5 concepts max, simple framing

**Always ask, always honor. Do NOT assume.**

---

### Rule 2: NO FORWARD REFERENCES + PART 4 APPROPRIATE LANGUAGE

**Never mention untaught concepts:**
- âŒ NO Chapter 30+ references
- âŒ NO "Specification-Driven Development" (not yet taught - that's Part 5+)
- âŒ NO "write a specification" (use "describe your intent" instead)
- âŒ NO professional SDD terminology for Part 4 students

**DO reference AI-Native Learning (appropriate for Part 4):**
- âœ… "Describe what your code should do using type hints..."
- âœ… "Ask your AI to explain this concept..."
- âœ… "Validate your understanding by testing the code..."
- âœ… "Learn from errors by asking AI 'why did this fail?'..."

**DO reference AIDD principles from Chapters 1-11 (context only):**
- âœ… "This applies the AIDD thinking you learned in Part 1..."
- âœ… "Remember the validation-first approach from Chapter 4..."
- âœ… "You're using AI as co-reasoning partner, not coding assistant..."

**Critical Distinction**:
- Part 4 students use **AI-Native Learning** (beginner-friendly: describe intent â†’ explore â†’ validate â†’ learn from errors)
- Part 5+ students learn **Specification-Driven Development** (professional: write formal specs â†’ generate â†’ test â†’ iterate)
- Type hints are "describing intent" NOT "writing specifications" in Part 4

---

### Rule 3: RUTHLESS CONTEXT FILTERING

**When extracting from context materials:**

**Chapter 13 "Introduction to Python":**
- âœ… "What is Python?" â†’ USE (intro concept)
- âœ… "Your first program" â†’ USE (intro outcome)
- âŒ "Functions" â†’ SKIP (Ch 20 topic)
- âŒ "Classes" â†’ SKIP (Ch 24+ topic)
- âŒ "Async/await" â†’ SKIP (Ch 28 topic)

**Chapter 17 "Control Flow and Loops":**
- âœ… "if/elif/else statements" â†’ USE (chapter focus)
- âœ… "for loops" â†’ USE (chapter focus)
- âŒ "Functions" â†’ SKIP (Ch 20 topic)
- âŒ "List comprehensions" â†’ SKIP (advanced)
- âŒ "Exception handling" â†’ SKIP (Ch 21 topic)

**Decision Rule:**
- IF context concept fits THIS chapter's title â†’ EXTRACT
- IF context concept belongs to Ch N+1 or later â†’ SKIP
- IF context concept is advanced variation â†’ SKIP
- IF context concept requires future prerequisites â†’ SKIP

---

### Rule 4: MINIMAL SCOPE

**Depth > breadth.**

- Beginner (Ch 12-16): 5 concepts max, 3-4 lessons
- Intermediate (Ch 17-23): 7 concepts max, 4-5 lessons
- Advanced (Ch 24-29): 10 concepts max, 5-6 lessons

---

### Rule 5: MINIMAL FILES

**Create ONLY:**
- âœ… spec.md (what students learn)
- âœ… plan.md (how we teach it)
- âœ… tasks.md (implementation checklist)

**Never create:**
- âŒ index.md, _templates/, _assets/, _code-examples/, lesson-template.md, capstone-rubric.md

---

### Rule 6: TROUBLESHOOTING IS AI PARTNERSHIP

**Real-world context:** In an AI-native world, students will encounter errors (installation, syntax, environment issues). Rather than detailed troubleshooting in every chapter, teach students to ASK their AI assistant.

**Application in chapters:**
- **Installation/Setup chapters**: Include prompt like: `"I tried to install Python but got this error: [error]. What does this mean and how do I fix it?"`
- **Execution chapters**: Include prompt like: `"My program runs but gives this output. Is this correct? Why?"`
- **Advanced chapters**: Include prompt like: `"I'm getting a TypeError. Walk me through what went wrong."`

**Why this works:**
- âœ… Teaches resilience: Errors are information to be understood, not obstacles
- âœ… Builds partnership: AI becomes problem-solving collaborator, not just code generator
- âœ… Scales with complexity: Works for simple errors (Python not found) to complex errors (type mismatches)
- âœ… Honors reality: Professional developers ask AI for error help constantly

**Example (from Chapter 13, Lesson 2):**
```markdown
### Prompt 2: Troubleshoot Installation Errors
\`\`\`
I tried to install Python but got this error: [describe your error].
What does this mean and how do I fix it?
\`\`\`

**Expected outcome**: AI explains the error and provides step-by-step fixing instructions.
```

This single prompt replaces 10 pages of platform-specific troubleshooting guides that become outdated.

---

### Rule 7: GRADUATED TEACHING PATTERN (Constitution Principle 13)

**Apply the three-tier teaching approach from the constitution:**

**Tier 1 - Foundational Concepts** (Book Teaches Directly):
- Stable, core concepts explained directly in book
- Direct explanation with analogies and examples
- Examples: What are variables? What is a loop? What are type hints?
- NO "Ask your AI: What is X?" for foundations
- Book provides clear, authoritative explanation first

**Tier 2 - Complex Syntax** (AI Companion Handles):
- Complex syntax patterns AI handles (student directs, AI executes)
- Student specifies WHAT they want, AI handles HOW
- Examples: Decorators, context managers, complex regex, async/await patterns
- "Tell your AI: Create X with these requirements..."
- Student learns strategy and intent, not memorization of syntax

**Tier 3 - Scaling Operations** (AI Orchestration):
- Operations involving 10+ items or multi-file workflows
- Student orchestrates strategy, AI manages tactical execution
- Examples: Setting up 10 test environments, batch file conversions, project scaffolding
- "Tell your AI: Set up 10 X with Y configuration..."
- Student learns orchestration and supervision skills

**Application to Part 4 (Chapters 12-29)**:
- Primarily Tier 1 (foundations) and Tier 2 (applied syntax)
- Tier 3 introduced gradually in advanced chapters (24-29)
- Balance: Book teaches concepts, AI handles complexity, student directs strategy

---

### Rule 8: STANDARDIZED "TRY WITH AI" FORMAT

**Every lesson MUST end with "Try With AI" section** following this exact structure (verified in Chapter 1 and Chapter 13):

```markdown
## Try With AI

Use your AI companion (Claude Code or Gemini CLI). [Brief context about what you're exploring].

### Prompt 1: [Descriptive Title]
\`\`\`
[Clear, concrete prompt asking about the concept]
\`\`\`

**Expected outcome**: [What student should understand after AI response]

### Prompt 2: [Descriptive Title]
\`\`\`
[Clear, concrete prompt asking about application or edge case]
\`\`\`

**Expected outcome**: [What student learns from this]

### Prompt 3: [Descriptive Title]
\`\`\`
[Prompt encouraging deeper understanding or connection to real-world use]
\`\`\`

**Expected outcome**: [Connection to AIDD or professional practice]

### Prompt 4: [Descriptive Title]
\`\`\`
[Synthesis prompt pulling together concepts from lesson]
\`\`\`

**Expected outcome**: [Integration of understanding]
```

**Critical requirements:**
- âœ… Exactly 4 prompts per lesson (progressive complexity)
- âœ… Prompts are CONCRETE and SPECIFIC (not "ask AI about X")
- âœ… Each prompt has explicit "Expected outcome" describing what student learns
- âœ… Prompts should include rubric-style validation ("Does this answer your spec?")
- âœ… No "Key Takeaways" or "Summary" sections after "Try With AI"
- âœ… "Try With AI" is the final substantive section (closure point)

**CRITICAL LESSON CLOSURE PATTERN** (Constitutional Mandate):

Lessons MUST end with "Try With AI" section ONLY. Prompt 4 provides cognitive closure.

**NEVER ADD after "Try With AI":**
- âŒ "Key Takeaways" or "Summary"
- âŒ "What's Next"
- âŒ "Completion Checklist" (even for capstone lessons)
- âŒ "Chapter Summary"
- âŒ Any other closure content

**WHY**: Try With AI Prompt 4 already provides reflection and synthesis. Additional sections create cognitive overload and violate Constitutional Rule 13. This was identified as a critical violation in Chapter 14 technical review.

**Why this matters:**
- Consistency across entire book (students know the format)
- Progressive prompts teach exploration, not memorization
- "Expected outcome" sets clear learning targets
- Validates understanding without artificial quizzes
- Prompt 4 synthesis provides natural closure

---

## ORCHESTRATED WORKFLOW (What Actually Happens)

When you run `/sp.python-chapter [N]`:

### PHASE 0: Intelligent Context Gathering (Adaptive)

**Intelligence-Driven Discovery** (not hardcoded questions):

1. **Read authoritative sources**:
   - Constitution: `.specify/memory/constitution.md` (target audience, philosophy, principles)
   - Chapter Index: `specs/book/chapter-index.md` (chapter title, part, prerequisite chapters)
   - Skills Library: `.claude/skills/` (available domain skills)
   - Existing Context: `context/part-4-python/` or `context/13_chap12_to_29_specs/` (if available)

2. **Derive chapter intelligence**:
   - **Audience**: From constitution (Aspiring/Professional/Founders with graduated complexity)
   - **Part**: From chapter-index.md (chapter N â†’ Part X)
   - **Complexity Tier**: From chapter number range (12-16=beginner, 17-23=intermediate, 24-29=advanced)
   - **Prerequisite Knowledge**: All chapters 1 through N-1
   - **Chapter Title**: Exact title from chapter-index.md (THE ANCHOR)
   - **Learning Pattern**: AI-Native Learning (Part 4 appropriate, NOT formal SDD)

3. **Intelligently determine what to ask user** (context-adaptive):
   - IF context materials exist for this chapter â†’ Ask: "Use existing context or start fresh?"
   - IF chapter title is ambiguous/broad â†’ Ask: "What specific aspect should we emphasize?"
   - IF capstone vs foundational unclear â†’ Ask: "Should students BUILD something or learn concepts?"
   - IF multiple teaching approaches possible â†’ Ask: "Which pedagogical angle fits best?"

   **DO NOT ask**:
   - âŒ "Who is the audience?" (constitution already defines this)
   - âŒ "How many lessons?" (let intelligence determine based on scope)
   - âŒ "What CEFR level?" (derive from chapter number range automatically)

4. **Store derived intelligence** for next phases

**Apply Vertical Intelligence**: Constitution + Chapter Index + Skills â†’ Adaptive questions (not hardcoded forms).

---

### PHASE 1: Specification (Automated)

```
â†’ Invoke: /sp.specify [chapter-context]
  â”œâ”€ Pass: chapter number, title, user answers, context materials
  â”œâ”€ Apply: AI-Native Learning principles, cognitive load limits, teaching patterns
  â”œâ”€ Create: specs/part-4-chapter-[N]/spec.md
  â””â”€ Report: "Spec created. Review and approve."

WAIT: User reviews spec.md
â†’ User confirms: "âœ… Spec approved" or provides feedback
  â”œâ”€ If feedback: Update spec.md iteratively
  â””â”€ If approved: Continue to PHASE 2
```

**What /sp.specify receives:**
- Chapter title (anchor from chapter-index.md)
- User's audience answer (determines complexity tier: A1/A2/B1)
- User's scope answer (limits concepts to 5/7/10)
- User's outcome answer (real thing students will build)
- Context materials (extracted pedagogically)
- AI-Native Learning pattern (describe intent â†’ explore â†’ validate â†’ learn from errors)
- Teaching pattern template (What it is â†’ Code â†’ Try â†’ Why it matters)
- Cognitive load limits (max 5 for beginner, 7 for intermediate, 10 for advanced)

---

### PHASE 2: Planning (Automated)

```
â†’ Invoke: /sp.plan [spec-context]
  â”œâ”€ Read: specs/part-4-chapter-[N]/spec.md
  â”œâ”€ Apply: Lesson progression, CEFR proficiency levels, AI prompts, skills-proficiency-mapper
  â”œâ”€ Create: specs/part-4-chapter-[N]/plan.md
  â””â”€ Report: "Plan created. Review and approve."

WAIT: User reviews plan.md
â†’ User confirms: "âœ… Plan approved" or provides feedback
  â”œâ”€ If feedback: Update plan.md iteratively
  â””â”€ If approved: Continue to PHASE 3
```

**What /sp.plan receives:**
- Approved spec.md (learning objectives, concepts, success criteria)
- Chapter scope (what fits this chapter, what doesn't)
- AI-Native Learning pattern (Describe intent â†’ Explore â†’ Validate â†’ Learn from errors)
- Proficiency expectations (CEFR A1/A2/B1 levels)
- Real outcome students will build
- Skills proficiency mapper for CEFR validation and cognitive load checks

---

### PHASE 3: Tasks (Automated)

```
â†’ Invoke: /sp.tasks [spec+plan-context]
  â”œâ”€ Read: specs/part-4-chapter-[N]/spec.md + plan.md
  â”œâ”€ Apply: Acceptance criteria, validation steps, implementation checklist
  â”œâ”€ Create: specs/part-4-chapter-[N]/tasks.md
  â””â”€ Report: "Tasks created. Review and approve."

WAIT: User reviews tasks.md
â†’ User confirms: "âœ… Tasks approved" or provides feedback
  â”œâ”€ If feedback: Update tasks.md iteratively
  â””â”€ If approved: Continue to PHASE 4
```

**What /sp.tasks receives:**
- Approved spec.md + plan.md (complete design)
- Learning objectives (what success looks like)
- Lessons (what needs to be implemented)
- Acceptance criteria (how to validate)

---

### PHASE 4: Implementation (Optional)

```
â†’ Ask user: "Ready to implement lesson content?"

Options:
A) Implement with lesson-writer subagent
   â†’ Invoke: lesson-writer subagent
   â†’ Pass: spec.md, plan.md, tasks.md
   â†’ Apply: AI-Native Learning pattern, CEFR levels, validation-first approach
   â†’ Create: docs/part-4/chapter-[N]/{01,02,03,04}-lesson-*.md
   â†’ Then: Invoke technical-reviewer for validation

B) Manual implementation
   â†’ User implements using tasks.md as checklist

C) Done for now
   â†’ Keep design artifacts, skip implementation

â†’ Report final status
```

---

## KEY PRINCIPLES (Always Applied)

### âœ… AI-Native Learning First (Part 4 Appropriate)
- Apply AI-Native Learning pattern: describe intent â†’ explore â†’ validate â†’ learn from errors
- Reference AIDD principles from Chapters 1-11 for context (not formal methodology)
- Validation-first practice: "How will students test understanding?"
- AI partnership: "How will they use Claude Code/Gemini CLI as co-reasoning partners?"
- NO formal "specification writing" (that's Part 5+) - use "describe intent" framing

### âœ… No Forward References
- Zero mentions of Chapters 30+ (SDD taught later)
- No "Specification-Driven Development" terminology (use "AI-Native Learning")
- No concepts from future chapters
- Chapter title from `chapter-index.md` is the absolute anchor

### âœ… Honors User Intent
- User's audience choice = final decision (never override)
- User's scope answer = limits concepts (never expand)
- User's outcome answer = determines lessons (never modify)

### âœ… Ruthless Context Filtering
- Only extract context matching THIS chapter's title
- Skip concepts from future chapters (even if in materials)
- Skip advanced variations and tangential concepts

### âœ… Cognitive Load Limits
- Max 5 concepts for beginner (Ch 12-16)
- Max 7 concepts for intermediate (Ch 17-23)
- Max 10 concepts for advanced (Ch 24-29)

### âœ… Teaching Intelligence Preserved
- Every phase applies AI-Native Learning principles
- Every phase uses teaching patterns (Book â†’ AI Companion â†’ AI Orchestration)
- Every phase respects chapter boundaries
- Every phase validates against acceptance criteria
- Skills proficiency mapping applied in planning phase (CEFR levels, cognitive load)

---

## EXECUTION INSTRUCTIONS (For Claude Code)

**CRITICAL**: This is an EXECUTABLE orchestration prompt, not documentation. Claude Code must:
1. Follow this flow exactly, in this order
2. Automatically invoke downstream commands WITHOUT asking for approval first
3. Pass full context (AIDD principles, teaching patterns) to each command
4. Respect approval gates ONLY between phases (not before first invocation)

### THE ORCHESTRATED WORKFLOW (EXECUTABLE)

#### PHASE 0: Intelligent Context Discovery (Adaptive, NOT Hardcoded)

**1. Read Authoritative Sources** (Automatic, NO USER INTERACTION):

```bash
# Constitution for audience, philosophy, principles
constitution=$(cat .specify/memory/constitution.md)

# Chapter index for title, part, prerequisites
chapter_data=$(grep "^| $CHAPTER_NUM |" specs/book/chapter-index.md)
chapter_title=$(echo "$chapter_data" | awk -F'|' '{print $3}' | tr im)
chapter_file=$(echo "$chapter_data" | awk -F'|' '{print $4}' | sed 's/`//g' | trim)

# Skills available
skills=$(ls -1 .claude/skills/)

# Context materials (if exist)
context_files=$(find context/ -name "*chapter-$CHAPTER_NUM*" 2>/dev/null)
```

**2. Derive Chapter Intelligence** (Automatic computation):

```python
# From constitution (no need to ask user)
audience = "Aspiring/Professional/Founders (graduated complexity)"

# From chapter number (automatic tier assignment)
if 12 <= chapter_num <= 16:
    complexity_tier = "beginner"
    cefr_range = "A1-A2"
    max_concepts = 5
elif 17 <= chapter_num <= 23:
    complexity_tier = "intermediate"
    cefr_range = "A2-B1"
    max_concepts = 7
elif 24 <= chapter_num <= 29:
    complexity_tier = "advanced"
    cefr_range = "B1-B2"
    max_concepts = 10

# From chapter index (THE ANCHOR)
part_num = 4  # Chapters 12-29 are Part 4
prerequisites = f"Chapters 1-{chapter_num - 1}"
learning_pattern = "AI-Native Learning"  # Part 4 appropriate

# Store derived intelligence
chapter_intelligence = {
    "number": chapter_num,
    "title": chapter_title,  # FROM CHAPTER-INDEX.MD (authoritative)
    "part": part_num,
    "complexity_tier": complexity_tier,
    "cefr_range": cefr_range,
    "max_concepts_per_lesson": max_concepts,
    "prerequisites": prerequisites,
    "audience": audience,
    "learning_pattern": learning_pattern,
    "available_skills": skills
}
```

**3. Intelligently Determine What to Ask** (Context-adaptive):

```python
questions = []

# Only ask if genuinely ambiguous or requires human judgment
if context_files:
    questions.append("Existing context found for this chapter. Use it or start fresh?")

if chapter_title_is_broad(chapter_title):  # e.g., "Data Types" could mean many things
    questions.append(f"'{chapter_title}' - which specific aspects should we emphasize?")

if unclear_if_capstone(chapter_num, part_num):  # e.g., is this a build lesson?
    questions.append("Should students BUILD something or focus on concepts?")

# Ask only necessary questions (0-3 max, NOT hardcoded 4)
for q in questions:
    user_input = ask(q)
    chapter_intelligence["user_preferences"][q] = user_input
```

**4. Create Feature Branch** (Automatic, NO USER INTERACTION):

```bash
# Derive branch name from chapter data (not hardcoded)
branch_slug=$(echo "$chapter_file" | sed 's/\/$//; s/\`//g')  # e.g., "14-data-types"
git checkout -b "$branch_slug"
```

**Key Principle**: Intelligence derives from constitution + chapter-index + skills library. Only ask user when GENUINELY ambiguous or requires human creative judgment.

---

#### PHASE 1: Specification (Automated + Intelligent)

**THIS PHASE INVOKES `/sp.specify` AUTOMATICALLY WITH FULL CONTEXT**

1. **Prepare context** (Ruthless filtering applied):
   - Gather user's 4 answers from PHASE 0
   - Extract materials from context directories (if available):
     - `context/13_chap12_to_29_specs/` (legacy structure)
     - `context/part-4-python/` (preferred structure)
     - Skip if no context available (spec from scratch is valid)
   - Apply ruthless filtering: Skip future chapters, skip advanced variations, skip tangential concepts
   - Embed AI-Native Learning principles in the context
   - Embed teaching patterns in the context (Book â†’ AI Companion â†’ AI Orchestration)
   - Embed cognitive load limits (5 for beginner, 7 for intermediate, 10 for advanced)

2. **Invoke /sp.specify with full context**:
   ```
   /sp.specify [chapter-slug]

   Write Chapter [N]: [Title] in Part [P]

   [Full AIDD context, user answers, teaching patterns, cognitive load rules, ruthlessly filtered context materials]
   ```

3. **Wait for /sp.specify completion**:
   - âœ… `specs/part-[P]-chapter-[N]/spec.md` is created
   - âœ… AIDD principles applied
   - âœ… Teaching patterns specified
   - âœ… Learning objectives aligned with evals

4. **Output approval checkpoint**:
   ```
   âœ… PHASE 1 COMPLETE: Specification Created

   ğŸ“‹ specs/part-[P]-chapter-[N]/spec.md

   Please review and confirm:
   - âœ… "Spec approved" to proceed to planning
   - ğŸ“ Feedback to revise specification
   - â“ Questions for clarification
   ```

5. **Wait for user approval**: Block here until user confirms "âœ… Spec approved" OR provides feedback

---

#### PHASE 2: Planning (Automated + Intelligent) - Triggered After Spec Approval

**THIS PHASE INVOKES `/sp.plan` AUTOMATICALLY WITH FULL CONTEXT**

1. **Prepare context** (Read approved spec, add intelligence):
   - Read: `specs/part-[P]-chapter-[N]/spec.md` (the approved specification)
   - Extract: Learning objectives, key concepts, success criteria
   - Add: CEFR proficiency levels (A1/A2/B1 based on audience)
   - Add: Skills proficiency mapping (identify skills, assign CEFR levels, validate progression)
   - Add: Cognitive load validation (max concepts per lesson based on proficiency)
   - Add: Bloom's taxonomy alignment (cognitive complexity matching proficiency level)
   - Add: Lesson progression rules (foundational â†’ applied â†’ integration)
   - Add: AI prompts for each lesson (validation-first approach)
   - Add: Teaching pattern structure for every lesson (Book â†’ AI Companion â†’ AI Orchestration)

2. **Invoke /sp.plan with full context**:
   ```
   /sp.plan [chapter-slug]

   [Full context from spec, CEFR levels, lesson structure, AIDD teaching patterns]
   ```

3. **Wait for /sp.plan completion**:
   - âœ… `specs/part-[P]-chapter-[N]/plan.md` is created
   - âœ… Lessons broken down lesson-by-lesson
   - âœ… CEFR proficiency levels assigned
   - âœ… AI prompts specified

4. **Output approval checkpoint**:
   ```
   âœ… PHASE 2 COMPLETE: Plan Created

   ğŸ“‹ specs/part-[P]-chapter-[N]/plan.md

   Please review and confirm:
   - âœ… "Plan approved" to proceed to tasks
   - ğŸ“ Feedback to revise plan
   - â“ Questions for clarification
   ```

5. **Wait for user approval**: Block here until user confirms "âœ… Plan approved" OR provides feedback

---

#### PHASE 3: Tasks (Automated + Intelligent) - Triggered After Plan Approval

**THIS PHASE INVOKES `/sp.tasks` AUTOMATICALLY WITH FULL CONTEXT**

1. **Prepare context** (Read approved spec + plan, add validation):
   - Read: `specs/part-[P]-chapter-[N]/spec.md` (learning objectives)
   - Read: `specs/part-[P]-chapter-[N]/plan.md` (lesson structure)
   - Add: Acceptance criteria for each lesson
   - Add: Validation steps (how to test understanding)
   - Add: Implementation checklist (content requirements)

2. **Invoke /sp.tasks with full context**:
   ```
   /sp.tasks [chapter-slug]

   [Full context from spec + plan, acceptance criteria, validation steps]
   ```

3. **Wait for /sp.tasks completion**:
   - âœ… `specs/part-[P]-chapter-[N]/tasks.md` is created
   - âœ… Implementation checklist defined
   - âœ… Validation steps specified

4. **Output completion report**:
   ```
   âœ… ALL DESIGN ARTIFACTS COMPLETE

   ğŸ“‹ specs/part-[P]-chapter-[N]/spec.md
   ğŸ“‹ specs/part-[P]-chapter-[N]/plan.md
   ğŸ“‹ specs/part-[P]-chapter-[N]/tasks.md
   ```

5. **Ask for next step**:
   ```
   Ready to implement?

   A) Implement with lesson-writer subagent
   B) Manual implementation (use tasks.md as checklist)
   C) Done for now (keep designs, skip implementation)
   ```

---

#### PHASE 4: Implementation (Optional) - Triggered Only If User Chooses A

**THIS PHASE INVOKES lesson-writer subagent IF AND ONLY IF USER CHOOSES OPTION A**

1. **Prepare context** (Read all 3 approved artifacts):
   - Read: `specs/part-[P]-chapter-[N]/spec.md`
   - Read: `specs/part-[P]-chapter-[N]/plan.md`
   - Read: `specs/part-[P]-chapter-[N]/tasks.md`
   - Add: AIDD teaching pattern (What it is â†’ Code â†’ Try â†’ Why it matters)
   - Add: CEFR levels for validation
   - Add: Validation-first approach (test understanding before moving on)

2. **Invoke lesson-writer subagent** (Only if user chose Option A):
   ```
   Task(
       subagent_type="lesson-writer",
       prompt=prepare_lesson_writer_prompt(
           spec, plan, tasks,
           aidd_teaching_pattern=True,
           cefr_levels=True,
           validation_first=True
       )
   )
   ```

3. **Wait for lesson-writer completion**:
   - âœ… `docs/part-[P]/chapter-[N]/{01,02,03,04}-lesson-*.md` created
   - âœ… Full AI-Native Learning methodology applied
   - âœ… AI partnership approach emphasized

4. **Invoke technical-reviewer** (Automatic validation):
   ```
   Task(
       subagent_type="technical-reviewer",
       prompt=f"""
       Validate Chapter {N}: {Title} with special focus on:

       **AI-Native Learning Principles**:
       - 4-step pattern applied (describe intent â†’ explore â†’ validate â†’ learn from errors)
       - AI positioned as co-reasoning partner, not coding assistant
       - Students directing AI, not passive learners

       **Part 4 Appropriate Language**:
       - NO "Specification-Driven Development" terminology (that's Part 5+)
       - Use "describe intent" not "write specifications"
       - AI-Native Learning framing, not professional SDD

       **Lesson Closure Pattern**:
       - ALL lessons end with "Try With AI" section ONLY
       - NO "Key Takeaways", "Summary", "Checklist" after Try With AI
       - Prompt 4 provides cognitive closure

       **Technical Accuracy**:
       - All code runs on Python 3.14+
       - Modern type hints throughout (list[int], dict[str, float], X | None)
       - No security issues, no hardcoded secrets

       **Constitutional Compliance**:
       - All 9 domain skills applied
       - Graduated teaching pattern followed
       - CEFR proficiency levels appropriate
       - Cognitive load within limits

       Output: Validation report with PASS/CONDITIONAL PASS/FAIL verdict
       """
   )
   ```

5. **Apply critical fixes** (if validation identifies issues):
   - Critical issues: MUST fix before proceeding
   - Major issues: SHOULD fix for quality
   - Minor issues: Optional improvements
   - Re-run technical-reviewer after fixes

6. **Final report**:
   ```
   âœ… CHAPTER [N] VALIDATED AND COMPLETE

   ğŸ“š Lessons created: docs/part-[P]/chapter-[N]/
   ğŸ“‹ Validation report: VALIDATION_REPORT_CHAPTER_[N].md

   Next steps:
   - Review validation report
   - Test lessons interactively
   - Prepare for publication
   - Commit to git
   ```

---

### CRITICAL EXECUTION RULES

1. **Sequential Invocation**: Phases execute in order (0 â†’ 1 â†’ 2 â†’ 3 â†’ 4), never out of order
2. **Automatic Chaining**: Each phase automatically invokes the next command (no "ask user first")
3. **Approval Gates Only Between Phases**: User approves AFTER each phase completes, BEFORE next phase starts
4. **Context Preservation**: Each phase reads prior phase outputs and passes them forward
5. **Vertical Intelligence Embedded**: EVERY command invocation includes AIDD principles, teaching patterns, cognitive load rules
6. **Ruthless Filtering**: Materials from future chapters are SKIPPED, not extracted
7. **No User Override**: User intent (audience, scope, outcome) is NEVER overridden, only honored
8. **Feature Branch Creation**: Automatic checkout of feature branch in PHASE 0, before any other work
9. **All 3 Artifacts Required**: Spec, Plan, and Tasks must exist before implementation can proceed

---

## CRITICAL VALIDATION (Before Each Phase)

**PHASE 1 Validation** (before `/sp.specify`):
- âœ… Chapter number valid (12-29, Part 4 only)
- âœ… Chapter title matches `chapter-index.md`
- âœ… User's audience answer captured
- âœ… User's scope answer captured
- âœ… User's outcome answer captured
- âœ… Context will be ruthlessly filtered (skip future chapters)
- âœ… AI-Native Learning principles will be applied (NOT formal SDD)

**PHASE 2 Validation** (before `/sp.plan`):
- âœ… spec.md was created successfully
- âœ… Concept count â‰¤ tier limit (5/7/10 based on chapter range)
- âœ… No forward references (Chapters 30+ or SDD terminology)
- âœ… AI-Native Learning framing used (not formal SDD)
- âœ… Only Chapters 1-N are prerequisites
- âœ… Teaching pattern respected (Book â†’ AI Companion â†’ AI Orchestration)
- âœ… Skills proficiency mapping will be applied

**PHASE 3 Validation** (before `/sp.tasks`):
- âœ… plan.md was created successfully
- âœ… Lessons match spec's learning objectives
- âœ… Proficiency levels assigned (CEFR A1/A2/B1)
- âœ… Cognitive load validated (concepts per lesson within limits)
- âœ… AI prompts specified for each lesson (4 prompts, progressive)
- âœ… Validation points defined
- âœ… Lesson closure pattern specified (Try With AI ONLY)

**PHASE 4 Validation** (before lesson-writer):
- âœ… All 3 design files exist and are valid
- âœ… User chose implementation option
- âœ… Context filtered ruthlessly (no future chapters)
- âœ… AI-Native Learning principles embedded
- âœ… Graduated teaching pattern clear
- âœ… Ready for lesson content creation

**PHASE 4 Post-Implementation Validation** (technical-reviewer):
- âœ… All lessons implement AI-Native Learning pattern
- âœ… No SDD terminology used inappropriately
- âœ… Lesson closure pattern followed (Try With AI ONLY)
- âœ… Code quality validated (Python 3.14+, type hints)
- âœ… CEFR proficiency levels appropriate
- âœ… Constitutional compliance verified

---

## WHAT GETS CREATED

**By End of PHASE 3** (mandatory):
```
specs/part-4-chapter-[N]/
  â”œâ”€â”€ spec.md       (What students learn + AI-Native Learning principles)
  â”œâ”€â”€ plan.md       (How we teach it, lesson-by-lesson + CEFR levels + skills mapping)
  â””â”€â”€ tasks.md      (Implementation checklist + validation)
```

**By End of PHASE 4** (if Option A chosen):
```
book-source/docs/04-Part-4-Python-Fundamentals/[N]-[chapter-name]/
  â”œâ”€â”€ readme.md                    (Chapter overview and navigation)
  â”œâ”€â”€ 01-[lesson-name].md          (Lesson 1)
  â”œâ”€â”€ 02-[lesson-name].md          (Lesson 2)
  â”œâ”€â”€ 03-[lesson-name].md          (Lesson 3)
  â”œâ”€â”€ 04-[lesson-name].md          (Lesson 4, or more based on plan)
  â””â”€â”€ 05-[capstone-name].md        (Optional capstone lesson)

VALIDATION_REPORT_CHAPTER_[N].md   (Technical review results)
```

---

## EXAMPLE EXECUTION (What User Sees)

```
> /sp.python-chapter 14

âº PHASE 0: Intelligent Context Discovery

ğŸ“– Reading authoritative sources...
  âœ“ Constitution: Target audience, philosophy, principles
  âœ“ Chapter Index: Chapter 14 = "Data Types" (Part 4)
  âœ“ Skills Library: 13 skills available
  âœ“ Context: Found existing materials in context/13_chap12_to_29_specs/

ğŸ§  Deriving chapter intelligence...
  âœ“ Chapter: 14 - "Data Types"
  âœ“ Part: 4 (Python Fundamentals)
  âœ“ Complexity Tier: Beginner (Chapters 12-16)
  âœ“ CEFR Range: A1-A2 (max 5 concepts/lesson)
  âœ“ Prerequisites: Chapters 1-13
  âœ“ Audience: Aspiring/Professional/Founders (graduated)
  âœ“ Learning Pattern: AI-Native Learning (NOT formal SDD)

ğŸ¤” Analyzing what needs clarification...
  â†’ Existing context materials found

  Q: Use existing context or start from scratch?
  [User: Use existing but adapt for AI-Native Learning emphasis]

  â†’ Chapter title "Data Types" is broad (could be 10+ topics)

  Q: Which specific data types should we emphasize?
  [User: Core types only - int, float, str, bool, None + type hints]

  â†’ Checking if capstone needed...

  Q: Should students BUILD something hands-on?
  [User: Yes - Interactive Type Explorer program]

âœ… Intelligence gathered (2 automatic derivations + 3 targeted questions). Moving to PHASE 1...

---

âº PHASE 1: Specification (Automated)

Invoking: /sp.specify with chapter context...
Applying: AI-Native Learning principles, teaching patterns, cognitive load limits

[/sp.specify generates spec.md]

ğŸ“‹ Spec created: specs/part-4-chapter-14/spec.md

Please review the specification:
- Does this match your vision for Chapter 14?
- Are learning objectives clear?
- Is scope appropriate (not too broad)?
- Are AI-Native Learning principles emphasized?

Reply with: "âœ… Spec approved" or provide feedback

[User: âœ… Spec approved]

âœ… Spec approved. Moving to PHASE 2...

---

âº PHASE 2: Planning (Automated)

Invoking: /sp.plan with approved spec...
Applying: Lesson progression, CEFR levels, AI prompts, skills-proficiency-mapper

[/sp.plan generates plan.md with skills metadata]

ğŸ“‹ Plan created: specs/part-4-chapter-14/plan.md

Please review the lesson breakdown:
- Are 5 lessons appropriate (including capstone)?
- Are AI prompts specified for each lesson (4 prompts progressive)?
- Are CEFR proficiency levels correct (A2-B1 for data types)?
- Is AI-Native Learning pattern applied throughout?
- Are cognitive load limits respected?

Reply with: "âœ… Plan approved" or provide feedback

[User: âœ… Plan approved]

âœ… Plan approved. Moving to PHASE 3...

---

âº PHASE 3: Tasks (Automated)

Invoking: /sp.tasks with spec + plan...
Applying: Validation points, acceptance criteria, lesson structure

[/sp.tasks generates tasks.md]

ğŸ“‹ Tasks created: specs/part-4-chapter-14/tasks.md

Please review the implementation checklist:
- Are acceptance criteria clear?
- Are validation steps included (lesson closure pattern)?
- Are all 5 lessons covered with tasks?
- Are code testing tasks included?

Reply with: "âœ… Tasks approved" or provide feedback

[User: âœ… Tasks approved]

âœ… Tasks approved. Moving to PHASE 4...

---

âº PHASE 4: Implementation (Optional)

All design artifacts complete:
  âœ… specs/part-4-chapter-14/spec.md (AI-Native Learning)
  âœ… specs/part-4-chapter-14/plan.md (CEFR-leveled, skills mapped)
  âœ… specs/part-4-chapter-14/tasks.md (validation-first)

Ready to implement lesson content?

A) Implement with lesson-writer subagent + technical-reviewer validation
   â†’ Automatically creates lesson .md files with full AI-Native Learning methodology
   â†’ Then validates with technical-reviewer
B) Manual implementation
   â†’ You create lessons using tasks.md as guide
C) Done for now
   â†’ Keep design artifacts, implement later

[User chooses A]

âœ… Invoking lesson-writer subagent...
[5 lessons created in parallel/sequential approach]

âœ… Invoking technical-reviewer for validation...
[Validation report generated]

âš ï¸ CONDITIONAL PASS: 1 critical issue found
- Lesson 5 has checklist after "Try With AI" (violates closure pattern)

âœ… Applying fix...
[Removed checklist section from Lesson 5]

âœ… Re-validating...
[Validation passed]

---

âœ… WORKFLOW COMPLETE

Chapter 14 fully validated:
  âœ… Specification (AI-Native Learning principles)
  âœ… Lesson Plan (CEFR-leveled, skills mapped, cognitive load validated)
  âœ… Implementation Tasks (58/58 complete)
  âœ… Lesson Content (5 lessons, AI-Native Learning pattern applied)
  âœ… Technical Validation (PASS - all requirements met)

ğŸ“‹ Validation Report: VALIDATION_REPORT_CHAPTER_14.md

Next: Commit to git, prepare for publication
```

---

## CRITICAL SUCCESS FACTORS

1. **Automatic Invocation**: `/sp.specify`, `/sp.plan`, `/sp.tasks` must be invoked automatically via SlashCommand tool with full intelligence context

2. **Vertical Intelligence Preserved**: Every phase applies AIDD principles, teaching patterns, pedagogical design, and chapter boundary awareness

3. **Approval Gates**: User must explicitly approve each phase ("âœ… Approved") before proceeding to next

4. **Context Preservation**: Each phase receives full context from all previous phases + vertical intelligence

5. **Ruthless Filtering**: Context extraction must skip any concepts from future chapters, even if present in materials

6. **User Authority**: User's answers to 4 questions are final â€” never override with assumptions

7. **Compliance**: Every phase validates against acceptance criteria before proceeding

8. **Teaching Quality**: Intelligence flows through all 4 phases, not just documentation

---

## REFERENCES

- **Chapter Index**: `specs/book/chapter-index.md` (Part 4 Chapters: 12-29)
- **Constitution**: `.specify/memory/constitution.md` (AI-Native Learning principles, domain skills, graduated teaching pattern)
- **Skills Library**: `.claude/skills/` (skills-proficiency-mapper, learning-objectives, concept-scaffolding, etc.)
- **Context Materials**:
  - `context/13_chap12_to_29_specs/` (legacy structure)
  - `context/part-4-python/` (preferred structure)

---

## ONE COMMAND. FULL INTELLIGENCE. COMPLETE WORKFLOW.

Run `/sp.python-chapter [N]` and the system:

âœ… Gathers intelligent context (AI-Native Learning-driven questions)
âœ… Automatically chains `/sp.specify` â†’ `/sp.plan` â†’ `/sp.tasks` â†’ `/sp.implement` with approval gates
âœ… Applies vertical intelligence (AI-Native Learning, teaching patterns, pedagogy) at every phase
âœ… Respects chapter boundaries (ruthless context filtering, no forward references)
âœ… Honors user intent (never overrides audience/scope/outcome decisions)
âœ… Validates quality (acceptance criteria at each gate)
âœ… Implements lessons with lesson-writer subagent
âœ… Validates with technical-reviewer (AI-Native Learning compliance, lesson closure pattern)

**Result: AI-Native Learning-centered Python chapters ready for publication.**

---

**Note**: For PHR (Prompt History Record) creation after command completion, see constitution for instructions.
